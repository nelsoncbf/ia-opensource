Claro, Nelson! Aqui estÃ¡ o conteÃºdo completo em **Markdown**, pronto para copiar e colar no seu repositÃ³rio como `README.md`:

---

```markdown
# ğŸ§  RAG Open Source com Ollama, ChromaDB e LangChain

Este projeto implementa um pipeline de RecuperaÃ§Ã£o Aumentada por GeraÃ§Ã£o (RAG) utilizando tecnologias open source para criar uma aplicaÃ§Ã£o de IA local, privada e extensÃ­vel. Ele integra:

- **Ollama** para execuÃ§Ã£o de modelos LLM localmente
- **ChromaDB** como banco vetorial para armazenamento e busca semÃ¢ntica
- **LangChain** para orquestraÃ§Ã£o da lÃ³gica de RAG
- **Streamlit** como interface web interativa

Claro, Nelson! Aqui vai uma explicaÃ§Ã£o mais acessÃ­vel e didÃ¡tica de cada componente do seu projeto, ideal para quem nÃ£o tem familiaridade com IA ou desenvolvimento:

---

## ğŸ§  Componentes do Projeto RAG

### 1. **Ollama â€“ Motor de InteligÃªncia Artificial Local**

O Ollama Ã© como o "cÃ©rebro pensante" do sistema.  
Ele roda modelos de linguagem (LLMs) diretamente no seu computador, sem depender da internet ou de servidores externos. Esses modelos sÃ£o capazes de entender perguntas, gerar respostas, resumir textos e muito mais â€” tudo de forma privada e rÃ¡pida.

ğŸ”§ Exemplo de modelos que vocÃª pode usar: `llama2`, `mistral`, `gemma`.

---

### 2. **ChromaDB â€“ Banco de Dados Inteligente**

O ChromaDB Ã© um banco de dados especializado em lidar com **textos e significados**.  
Ao invÃ©s de guardar informaÃ§Ãµes como uma planilha, ele transforma os textos em vetores (nÃºmeros que representam o sentido das palavras) e permite fazer buscas por **similaridade de conteÃºdo**.

ğŸ’¡ Isso significa que, mesmo que vocÃª nÃ£o use as palavras exatas, ele entende o que vocÃª quer dizer e encontra os documentos mais relevantes.

---

### 3. **LangChain â€“ Orquestrador da IA**

O LangChain Ã© como o maestro da orquestra.  
Ele conecta todas as partes: pega os documentos, transforma em vetores, consulta o banco ChromaDB, envia a pergunta para o modelo do Ollama e junta tudo para gerar uma resposta inteligente.

ğŸ¯ Ele permite criar fluxos de raciocÃ­nio complexos, como:  
â€œBusque os documentos mais parecidos com a pergunta â†’ envie para o modelo â†’ gere uma resposta baseada nesses documentos.â€

---

### 4. **Streamlit â€“ Interface Visual para o UsuÃ¡rio**

O Streamlit Ã© a "cara" do projeto â€” a interface que vocÃª vÃª no navegador.  
Ele transforma o cÃ³digo Python em uma pÃ¡gina web interativa, onde vocÃª pode:

- Fazer perguntas
- Ver os documentos carregados
- Acompanhar as respostas da IA em tempo real

ğŸŒ Tudo isso acessando [http://localhost:8501](http://localhost:8501) no seu navegador.

---

## ğŸš€ Como subir a aplicaÃ§Ã£o

Certifique-se de ter o Docker e Docker Compose instalados.  
Depois, basta executar:

```bash
docker-compose up --build
```

Isso irÃ¡ subir os trÃªs serviÃ§os principais:

| ServiÃ§o   | Porta | DescriÃ§Ã£o |
|-----------|-------|-----------|
| `ollama`  | 11434 | Servidor local de modelos LLM via Ollama |
| `chromadb`| 8000  | Banco vetorial para armazenamento de embeddings |
| `app`     | 8501  | Interface web via Streamlit + lÃ³gica RAG |

Acesse a aplicaÃ§Ã£o em: [http://localhost:8501](http://localhost:8501)

---

## ğŸ§© Estrutura dos serviÃ§os

### 1. `ollama`

- **Imagem:** `ollama/ollama:latest`
- **FunÃ§Ã£o:** Executa modelos LLM localmente (ex: `llama2`, `mistral`, `gemma`)
- **Porta:** `11434`
- **PersistÃªncia:** Volume `ollama_data` para armazenar os modelos baixados

### 2. `chromadb`

- **Imagem:** `chromadb/chroma:latest`
- **FunÃ§Ã£o:** Armazena vetores de embeddings e realiza buscas semÃ¢nticas
- **Porta:** `8000`
- **PersistÃªncia:** Volume local `./chroma`

### 3. `app`

- **Build local:** Usa o Dockerfile do projeto
- **FunÃ§Ã£o:** Carrega documentos, gera embeddings, consulta o banco vetorial e interage com o modelo LLM
- **Porta:** `8501`
- **Volumes:**
  - `./data` â†’ pasta onde os documentos devem ser inseridos
  - `./chroma` â†’ compartilhamento com o banco vetorial

---

## ğŸ“‚ InserÃ§Ã£o de documentos

Coloque seus arquivos na pasta `./data`.  
Por padrÃ£o, o sistema carrega arquivos `.txt`, mas vocÃª pode adicionar suporte a:

- `.pdf` â†’ via `PyPDFLoader`
- `.docx` â†’ via `UnstructuredWordDocumentLoader`
- `.csv` â†’ via `CSVLoader`
- `.md` â†’ via `TextLoader`

Exemplo de instalaÃ§Ã£o de dependÃªncias no `requirements.txt`:

```
langchain
chromadb
pypdf        # dependÃªncia .pdf
unstructured # dependencia .docx
streamlit
```

---

## ğŸ› ï¸ PersonalizaÃ§Ã£o

VocÃª pode editar o `app.py` para:

- Escolher o modelo do Ollama (`llama2`, `mistral`, etc.)
- Ajustar o tipo de loader para suportar mais formatos
- Customizar o prompt de consulta
- Alterar o comportamento da interface Streamlit

---

## ğŸ“Œ Requisitos

- Docker + Docker Compose
- Modelos Ollama jÃ¡ baixados (ou serÃ£o baixados na primeira execuÃ§Ã£o)
- Documentos na pasta `data/` para alimentar o pipeline RAG

---

## ğŸ“ Suporte

Se tiver dÃºvidas ou quiser expandir o projeto, sinta-se Ã  vontade para abrir uma issue ou contribuir com melhorias!
```
